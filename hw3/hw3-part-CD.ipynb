{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qj3WkGyWsvfj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "import copy\n",
    "import csv\n",
    "import os\n",
    "from io import StringIO\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from torch.distributions import categorical\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "#from google.colab import files\n",
    "import matplotlib.pyplot as plt\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCZwXydseGyN"
   },
   "source": [
    "# Before you start\n",
    "You need to save a copy in your own Google Drive then you could edit on this colab.\n",
    "\n",
    "Google offers free GPU in the colab environments, but you may need to configure the environment.\n",
    "\n",
    "You can turn on the GPU mode in `Edit -> Notebook Settings` and change the `Runtime type` to be `Python3` and `Hardware accelerator` to be `GPU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aUyrvJPYM4tQ",
    "outputId": "db32a617-a318-461c-f074-0c3730a12d70"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU Model: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_name\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should get either a Tesla P100 or Tesla T4 GPU.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesla P100 is probably 3x faster than T4 but both should work.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/CS2078/lib/python3.10/site-packages/torch/cuda/__init__.py:423\u001b[0m, in \u001b[0;36mget_device_name\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_name\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    412\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the name of a device.\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \n\u001b[1;32m    414\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;124;03m        str: the name of the device\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[0;32m~/mambaforge/envs/CS2078/lib/python3.10/site-packages/torch/cuda/__init__.py:453\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_properties\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[1;32m    444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the properties of a device.\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \n\u001b[1;32m    446\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 453\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
      "File \u001b[0;32m~/mambaforge/envs/CS2078/lib/python3.10/site-packages/torch/cuda/__init__.py:302\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    301\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 302\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    306\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "print(\"GPU Model: %s\" % torch.cuda.get_device_name(0))\n",
    "print(\"You should get either a Tesla P100 or Tesla T4 GPU.\")\n",
    "print(\"Tesla P100 is probably 3x faster than T4 but both should work.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EV09-LqPFxzD"
   },
   "outputs": [],
   "source": [
    "PADDING_TOKEN = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpvbH0YSRE99"
   },
   "source": [
    "# RNN modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P22Z9p3LKSBb"
   },
   "outputs": [],
   "source": [
    "class GRUCell(nn.Module):\n",
    "  \"\"\"Implementation of GRU cell from https://arxiv.org/pdf/1406.1078.pdf.\"\"\"\n",
    "\n",
    "  def __init__(self, input_size, hidden_size, bias=False):\n",
    "    super().__init__()\n",
    "\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.bias = bias\n",
    "\n",
    "    # Learnable weights and bias for `update gate`\n",
    "    self.W_z = nn.Parameter(torch.Tensor(hidden_size, hidden_size + input_size))\n",
    "    if bias:\n",
    "      self.b_z = nn.Parameter(torch.Tensor(hidden_size))\n",
    "    else:\n",
    "      self.register_parameter('b_z', None)\n",
    "\n",
    "    # Learnable weights and bias for `reset gate`\n",
    "    self.W_r = nn.Parameter(torch.Tensor(hidden_size, hidden_size + input_size))\n",
    "    if bias:\n",
    "      self.b_r = nn.Parameter(torch.Tensor(hidden_size))\n",
    "    else:\n",
    "      self.register_parameter('b_r', None)\n",
    "\n",
    "    # Learnable weights and bias for `output gate`\n",
    "    self.W = nn.Parameter(torch.Tensor(hidden_size, hidden_size + input_size))\n",
    "    if bias:\n",
    "      self.b = nn.Parameter(torch.Tensor(hidden_size))\n",
    "    else:\n",
    "      self.register_parameter('b', None)\n",
    "\n",
    "    self.reset_parameters()\n",
    "\n",
    "  def forward(self, x, prev_state):\n",
    "    if prev_state is None:\n",
    "      batch = x.shape[0]\n",
    "      prev_h = torch.zeros((batch, self.hidden_size), device=x.device)\n",
    "    else:\n",
    "      prev_h = prev_state\n",
    "\n",
    "    concat_hx = torch.cat((prev_h, x), dim=1)\n",
    "    z = torch.sigmoid(F.linear(concat_hx, self.W_z, self.b_z))\n",
    "    r = torch.sigmoid(F.linear(concat_hx, self.W_r, self.b_r))\n",
    "    h_tilde = torch.tanh(\n",
    "        F.linear(torch.cat((r * prev_h, x), dim=1), self.W, self.b))\n",
    "    next_h = (1 - z) * prev_h + z * h_tilde\n",
    "    return next_h\n",
    "\n",
    "  def reset_parameters(self):\n",
    "    sqrt_k = (1. / self.hidden_size)**0.5\n",
    "    with torch.no_grad():\n",
    "      for param in self.parameters():\n",
    "        param.uniform_(-sqrt_k, sqrt_k)\n",
    "    return\n",
    "\n",
    "  def extra_repr(self):\n",
    "    return 'input_size={}, hidden_size={}, bias={}'.format(\n",
    "        self.input_size, self.hidden_size, self.bias is not True)\n",
    "\n",
    "  def count_parameters(self):\n",
    "    print('Total Parameters: %d' %\n",
    "          sum(p.numel() for p in self.parameters() if p.requires_grad))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HTbx2lMiKYIs"
   },
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size, hidden_size, bias=False):\n",
    "    super().__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.bias = bias\n",
    "\n",
    "    #####################################################################\n",
    "    # Implement here following the given signature                      #\n",
    "    raise NotImplementedError\n",
    "    #####################################################################\n",
    "\n",
    "    return\n",
    "\n",
    "  def forward(self, x, prev_state):\n",
    "    #####################################################################\n",
    "    # Implement here following the given signature                      #\n",
    "    raise NotImplementedError\n",
    "    #####################################################################\n",
    "    return\n",
    "\n",
    "  def reset_parameters(self):\n",
    "    sqrt_k = (1. / self.hidden_size)**0.5\n",
    "    with torch.no_grad():\n",
    "      for param in self.parameters():\n",
    "        param.uniform_(-sqrt_k, sqrt_k)\n",
    "    return\n",
    "\n",
    "  def extra_repr(self):\n",
    "    return 'input_size={}, hidden_size={}, bias={}'.format(\n",
    "        self.input_size, self.hidden_size, self.bias is not True)\n",
    "\n",
    "  def count_parameters(self):\n",
    "    print('Total Parameters: %d' %\n",
    "          sum(p.numel() for p in self.parameters() if p.requires_grad))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ud8tBquKd61"
   },
   "outputs": [],
   "source": [
    "class PeepholedLSTMCell(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size, hidden_size, bias=False):\n",
    "    super().__init__()\n",
    "\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.bias = bias\n",
    "\n",
    "    #####################################################################\n",
    "    # Implement here following the given signature                      #\n",
    "    raise NotImplementedError\n",
    "    #####################################################################\n",
    "\n",
    "    return\n",
    "\n",
    "  def forward(self, x, prev_state):\n",
    "    #####################################################################\n",
    "    # Implement here following the given signature                      #\n",
    "    raise NotImplementedError\n",
    "    #####################################################################\n",
    "    return\n",
    "\n",
    "  def reset_parameters(self):\n",
    "    sqrt_k = (1. / self.hidden_size)**0.5\n",
    "    with torch.no_grad():\n",
    "      for param in self.parameters():\n",
    "        param.uniform_(-sqrt_k, sqrt_k)\n",
    "    return\n",
    "\n",
    "  def extra_repr(self):\n",
    "    return 'input_size={}, hidden_size={}, bias={}'.format(\n",
    "        self.input_size, self.hidden_size, self.bias is not True)\n",
    "\n",
    "  def count_parameters(self):\n",
    "    print('Total Parameters: %d' %\n",
    "          sum(p.numel() for p in self.parameters() if p.requires_grad))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J4nKRsHAKkwW"
   },
   "outputs": [],
   "source": [
    "class CoupledLSTMCell(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size, hidden_size, bias=False):\n",
    "    super().__init__()\n",
    "\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.bias = bias\n",
    "\n",
    "    #####################################################################\n",
    "    # Implement here following the given signature                      #\n",
    "    raise NotImplementedError\n",
    "    #####################################################################\n",
    "\n",
    "    return\n",
    "\n",
    "  def forward(self, x, prev_state):\n",
    "    #####################################################################\n",
    "    # Implement here following the given signature                      #\n",
    "    raise NotImplementedError\n",
    "    #####################################################################\n",
    "    return\n",
    "\n",
    "  def reset_parameters(self):\n",
    "    sqrt_k = (1. / self.hidden_size)**0.5\n",
    "    with torch.no_grad():\n",
    "      for param in self.parameters():\n",
    "        param.uniform_(-sqrt_k, sqrt_k)\n",
    "    return\n",
    "\n",
    "  def extra_repr(self):\n",
    "    return 'input_size={}, hidden_size={}, bias={}'.format(\n",
    "        self.input_size, self.hidden_size, self.bias is not True)\n",
    "\n",
    "  def count_parameters(self):\n",
    "    print('Total Parameters: %d' %\n",
    "          sum(p.numel() for p in self.parameters() if p.requires_grad))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plW2UsQlSGIl"
   },
   "outputs": [],
   "source": [
    "RNN_MODULES = {\n",
    "  'gru': GRUCell,\n",
    "  'lstm': LSTMCell,\n",
    "  'peepholed_lstm': PeepholedLSTMCell,\n",
    "  'coupled_lstm': CoupledLSTMCell,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1-I1OaIXPAx"
   },
   "source": [
    "# Upload data\n",
    "Please use the following code snippet to upload\n",
    "\n",
    "* imdb_train.csv\n",
    "* imdb_test.csv\n",
    "* shakespeare.txt\n",
    "\n",
    "You can choose multiple files to upload all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "2WvA8lDXPlOQ",
    "outputId": "3e0feeb4-182e-490b-9dc6-887410087169"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-2b757ce3-adb1-458f-a11d-bf34762cad55\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-2b757ce3-adb1-458f-a11d-bf34762cad55\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving imdb_test.csv to imdb_test.csv\n",
      "Saving imdb_train.csv to imdb_train.csv\n",
      "Saving shakespeare.txt to shakespeare.txt\n",
      "User uploaded file \"imdb_test.csv\" with length 6640779 bytes\n",
      "User uploaded file \"imdb_train.csv\" with length 60197565 bytes\n",
      "User uploaded file \"shakespeare.txt\" with length 1115394 bytes\n"
     ]
    }
   ],
   "source": [
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSI42Iv7U9iL"
   },
   "outputs": [],
   "source": [
    "train_dataset_text = uploaded['imdb_train.csv']\n",
    "test_dataset_text = uploaded['imdb_test.csv']\n",
    "shakespeare_text = uploaded['shakespeare.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQ8MqD2HQ5SP"
   },
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3eWmqa4OSkVY"
   },
   "outputs": [],
   "source": [
    "### Hyperparameters for training (previously defined in FLAGS)\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 0\n",
    "BATCH_SIZE = 4096\n",
    "EPOCHS = 100\n",
    "GRADIENT_CLIP_NORM = 1.0\n",
    "\n",
    "### Hyperparameters for sentence analysis model\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_SIZE = 100\n",
    "REVIEW_MAX_LENGTH = 200\n",
    "VOCABULARY_MIN_COUNT = 100\n",
    "VOCABULARY_MAX_SIZE = 20000\n",
    "RNN_MODULE = 'gru'    # You need to try 'lstm', 'peepholed_lstm', 'coupled_lstm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mMoTuAy3ROgs"
   },
   "outputs": [],
   "source": [
    "class IMDBReviewDataset(Dataset):\n",
    "\n",
    "  def __init__(self,\n",
    "               csv_text,\n",
    "               vocabulary=None,\n",
    "               vocab_min_count=10,\n",
    "               vocab_max_size=None,\n",
    "               review_max_length=200):\n",
    "    self.csv_text = csv_text\n",
    "    self.vocab_min_count = vocab_min_count\n",
    "    self.vocab_max_size = vocab_max_size\n",
    "    self.review_max_length = review_max_length - 2\n",
    "\n",
    "    self.data = []\n",
    "\n",
    "    encoded_text = csv_text.strip().decode(encoding='utf-8')\n",
    "    fp = StringIO(encoded_text)\n",
    "    reader = csv.DictReader(fp, delimiter=',')\n",
    "    for row in tqdm(reader):\n",
    "      self.data.append((row['review'].split(' ')[:review_max_length],\n",
    "                        int(row['sentiment'] == 'positive')))\n",
    "    fp.close()\n",
    "\n",
    "    if vocabulary is not None:\n",
    "      print('Using external vocabulary - vocab-related configs ignored.')\n",
    "      self.vocabulary = vocabulary\n",
    "    else:\n",
    "      self.vocabulary = self._build_vocabulary()\n",
    "\n",
    "    self.word2index = {w: i for (i, w) in enumerate(self.vocabulary)}\n",
    "    self.index2word = {i: w for (i, w) in enumerate(self.vocabulary)}\n",
    "    self.oov_token_id = self.word2index['OOV_TOKEN']\n",
    "    self.pad_token_id = self.word2index['PAD_TOKEN']\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    review, label = self.data[index]\n",
    "    review = ['BEGIN_TOKEN'] + review + ['END_TOKEN']\n",
    "    token_ids = [self.word2index.get(w, self.oov_token_id) for w in review]\n",
    "    return token_ids, label\n",
    "\n",
    "  def _build_vocabulary(self):\n",
    "    special_tokens = ['PAD_TOKEN', 'BEGIN_TOKEN', 'OOV_TOKEN', 'END_TOKEN']\n",
    "\n",
    "    counter = collections.Counter()\n",
    "    for review, _ in self.data:\n",
    "      counter.update(review)\n",
    "\n",
    "    vocab = counter.most_common(self.vocab_max_size - 4)\n",
    "    if self.vocab_min_count is not None:\n",
    "      vocab_tokens = [w for (w, c) in vocab if c >= self.vocab_min_count]\n",
    "    else:\n",
    "      vocab_tokens, _ = zip(vocab)\n",
    "\n",
    "    return special_tokens + vocab_tokens\n",
    "\n",
    "  def get_vocabulary(self):\n",
    "    return self.vocabulary\n",
    "\n",
    "  def print_statistics(self):\n",
    "    reviews, labels = zip(*self.data)\n",
    "    lengths = [len(x) for x in reviews]\n",
    "    positive = np.sum(labels)\n",
    "    negative = len(labels) - positive\n",
    "    print('Total instances: %d, positive: %d, negative: %d' %\n",
    "          (len(self.data), positive, negative))\n",
    "    print('Review lengths: max: %d, min: %d, mean: %d, median: %d' %\n",
    "          (max(lengths), min(lengths), np.mean(lengths), np.median(lengths)))\n",
    "    print('Vocabulary size: %d' % len(self.vocabulary))\n",
    "    return\n",
    "\n",
    "\n",
    "def imdb_collate_fn(batch_data, padding_token_id=PADDING_TOKEN):\n",
    "  \"\"\"Padding variable-length sequences.\"\"\"\n",
    "  batch_tokens, batch_labels = zip(*batch_data)\n",
    "  lengths = [len(x) for x in batch_tokens]\n",
    "  max_length = max(lengths)\n",
    "\n",
    "  padded_tokens = []\n",
    "  for tokens, length in zip(batch_tokens, lengths):\n",
    "    padded_tokens.append(tokens + [padding_token_id] * (max_length - length))\n",
    "\n",
    "  padded_tokens = torch.tensor(padded_tokens, dtype=torch.int64)\n",
    "  lengths = torch.tensor(lengths, dtype=torch.int64)\n",
    "  labels = torch.tensor(batch_labels, dtype=torch.int64)\n",
    "\n",
    "  return padded_tokens, lengths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PpEQBZJYPLbE"
   },
   "outputs": [],
   "source": [
    "class SentimentClassification(nn.Module):\n",
    "\n",
    "  def __init__(self,\n",
    "               vocabulary_size,\n",
    "               embedding_dim,\n",
    "               rnn_module,\n",
    "               hidden_size,\n",
    "               bias=False):\n",
    "    super().__init__()\n",
    "    self.vocabulary_size = vocabulary_size\n",
    "    self.rnn_module = rnn_module\n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.hidden_size = hidden_size\n",
    "    self.bias = bias\n",
    "\n",
    "    self.embedding = nn.Embedding(num_embeddings=vocabulary_size,\n",
    "                                  embedding_dim=embedding_dim,\n",
    "                                  padding_idx=PADDING_TOKEN)\n",
    "    self.rnn_model = self.rnn_module(input_size=embedding_dim,\n",
    "                                     hidden_size=hidden_size,\n",
    "                                     bias=bias)\n",
    "    self.classifier = nn.Linear(hidden_size, 2)\n",
    "    return\n",
    "\n",
    "  def forward(self, batch_reviews, batch_lengths):\n",
    "    data = self.embedding(batch_reviews)\n",
    "\n",
    "    state = None\n",
    "    batch_size, total_steps, _ = data.shape\n",
    "    full_outputs = []\n",
    "    for step in range(total_steps):\n",
    "      next_state = self.rnn_model(data[:, step, :], state)\n",
    "      if isinstance(next_state, tuple):\n",
    "        h, c = next_state\n",
    "        full_outputs.append(h)\n",
    "      else:\n",
    "        full_outputs.append(next_state)\n",
    "      state = next_state\n",
    "\n",
    "    full_outputs = torch.stack(full_outputs, dim=1)\n",
    "    outputs = full_outputs[torch.arange(batch_size), batch_lengths - 1, :]\n",
    "    logits = self.classifier(outputs)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IqrED-lZPeSr"
   },
   "outputs": [],
   "source": [
    "def imdb_trainer(batch_size, epochs):\n",
    "  train_dataset = IMDBReviewDataset(csv_text=train_dataset_text,\n",
    "                                    vocab_min_count=VOCABULARY_MIN_COUNT,\n",
    "                                    vocab_max_size=VOCABULARY_MAX_SIZE,\n",
    "                                    review_max_length=REVIEW_MAX_LENGTH)\n",
    "  train_dataset.print_statistics()\n",
    "  train_loader = DataLoader(train_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            num_workers=8,\n",
    "                            collate_fn=imdb_collate_fn)\n",
    "  vocabulary = train_dataset.get_vocabulary()\n",
    "\n",
    "  # Validation dataset should use the same vocabulary as the training set.\n",
    "  val_dataset = IMDBReviewDataset(csv_text=test_dataset_text,\n",
    "                                  vocabulary=vocabulary,\n",
    "                                  review_max_length=REVIEW_MAX_LENGTH)\n",
    "  val_dataset.print_statistics()\n",
    "  val_loader = DataLoader(val_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=8,\n",
    "                          collate_fn=imdb_collate_fn)\n",
    "\n",
    "  best_model = None\n",
    "  best_acc = 0.0\n",
    "\n",
    "  full_train_loss = []\n",
    "  full_train_accuracy = []\n",
    "  full_val_loss = []\n",
    "  full_val_accuracy = []\n",
    "\n",
    "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "  model = SentimentClassification(vocabulary_size=len(vocabulary),\n",
    "                                  embedding_dim=EMBEDDING_DIM,\n",
    "                                  rnn_module=RNN_MODULES[RNN_MODULE],\n",
    "                                  hidden_size=HIDDEN_SIZE)\n",
    "  model.to(device)\n",
    "\n",
    "  print('Model Architecture:\\n%s' % model)\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "  optimizer = torch.optim.Adam(model.parameters(),\n",
    "                               lr=LEARNING_RATE,\n",
    "                               weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    for phase in ('train', 'eval'):\n",
    "      if phase == 'train':\n",
    "        model.train()\n",
    "        dataset = train_dataset\n",
    "        data_loader = train_loader\n",
    "      else:\n",
    "        model.eval()\n",
    "        dataset = val_dataset\n",
    "        data_loader = val_loader\n",
    "\n",
    "      running_loss = 0.0\n",
    "      running_corrects = 0\n",
    "\n",
    "      for step, (reviews, lengths, labels) in tqdm(enumerate(data_loader)):\n",
    "        reviews = reviews.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(phase == 'train'):\n",
    "          outputs = model(reviews, lengths)\n",
    "          _, preds = torch.max(outputs, 1)\n",
    "          loss = criterion(outputs, labels)\n",
    "\n",
    "          if phase == 'train':\n",
    "            loss.backward()\n",
    "\n",
    "            # RNN model is easily getting exploded gradients, thus we perform\n",
    "            # gradients clipping to mitigate this issue.\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP_NORM)\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * reviews.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "      epoch_loss = running_loss / len(dataset)\n",
    "      epoch_acc = running_corrects.double() / len(dataset)\n",
    "      if phase == 'train':\n",
    "        full_train_accuracy.append(epoch_acc)\n",
    "        full_train_loss.append(epoch_loss)\n",
    "      elif phase == 'eval':\n",
    "        full_val_accuracy.append(epoch_acc)\n",
    "        full_val_loss.append(epoch_loss)\n",
    "\n",
    "      print('[Epoch %d] %s accuracy: %.4f, loss: %.4f' %\n",
    "            (epoch + 1, phase, epoch_acc, epoch_loss))\n",
    "\n",
    "      if phase == 'eval':\n",
    "        if epoch_acc > best_acc:\n",
    "          best_acc = epoch_acc\n",
    "          best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "  state_dict = {\"model\": best_model.cpu().state_dict(),\n",
    "                \"vocabulary\": vocabulary}\n",
    "  print(\"Best validation accuracy: %.4f\" % best_acc)\n",
    "  logs = (full_train_loss, full_train_accuracy, full_val_loss, full_val_accuracy)\n",
    "\n",
    "  return state_dict, logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "3Wzb82juVUkM",
    "outputId": "d47514a3-9a24-4838-b53b-1bfe75b04e00"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imdb_trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-82b61f2df2f5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimdb_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'imdb_trainer' is not defined"
     ]
    }
   ],
   "source": [
    "state_dict, logs = imdb_trainer(BATCH_SIZE, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aX0Jewz2iZ8r"
   },
   "outputs": [],
   "source": [
    "### You can make a plot using matplotlib with logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MoIIj56qRZkG"
   },
   "source": [
    "# Language model and sentence generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-A1T4jSYD90"
   },
   "outputs": [],
   "source": [
    "### Hyperparameters for training (previously defined in FLAGS)\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 0\n",
    "BATCH_SIZE = 4096\n",
    "EPOCHS = 10\n",
    "\n",
    "### Hyperparameters for sentence analysis model\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_SIZE = 512\n",
    "RNN_MODULE = 'gru'\n",
    "HISTORY_LENGTH = 100\n",
    "\n",
    "### Hyperparameters for generating new sentence\n",
    "GENERATION_LENGTH = 2000\n",
    "START_STRING = 'ROMEO'\n",
    "TEMPERATURE = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eU55O-zoC8Xp"
   },
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "\n",
    "  def __init__(self, encoded_text, history_length):\n",
    "    self.encoded_text = encoded_text\n",
    "    self.history_length = history_length\n",
    "\n",
    "    raw_text = self.encoded_text.strip().decode(encoding='utf-8')\n",
    "\n",
    "    self.vocab = sorted(set(raw_text))\n",
    "    self.char2index = {x: i for (i, x) in enumerate(self.vocab)}\n",
    "    self.index2char = {i: x for (i, x) in enumerate(self.vocab)}\n",
    "\n",
    "    self.data = [(raw_text[i:i + history_length], raw_text[i + history_length])\n",
    "                 for i in range(len(raw_text) - history_length)]\n",
    "    return\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    history, label = self.data[index]\n",
    "    history = np.array([self.char2index[x] for x in history])\n",
    "    label = self.char2index[label]\n",
    "    return history, label\n",
    "\n",
    "  def get_vocabulary(self):\n",
    "    return self.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6i53RH0ADB5L"
   },
   "outputs": [],
   "source": [
    "class SentenceGeneration(nn.Module):\n",
    "\n",
    "  def __init__(self,\n",
    "               vocabulary_size,\n",
    "               embedding_dim,\n",
    "               rnn_module,\n",
    "               hidden_size,\n",
    "               bias=False):\n",
    "    super().__init__()\n",
    "    self.vocabulary_size = vocabulary_size\n",
    "    self.rnn_module = rnn_module\n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.hidden_size = hidden_size\n",
    "    self.bias = bias\n",
    "\n",
    "    self.embedding = nn.Embedding(num_embeddings=vocabulary_size,\n",
    "                                  embedding_dim=embedding_dim,\n",
    "                                  padding_idx=PADDING_TOKEN)\n",
    "    self.rnn_model = self.rnn_module(input_size=embedding_dim,\n",
    "                                     hidden_size=hidden_size,\n",
    "                                     bias=bias)\n",
    "    self.classifier = nn.Linear(hidden_size, vocabulary_size)\n",
    "    return\n",
    "\n",
    "  def forward(self, batch_reviews, state=None):\n",
    "    data = self.embedding(batch_reviews)\n",
    "\n",
    "    batch_size, total_steps, _ = data.shape\n",
    "    for step in range(total_steps):\n",
    "      next_state = self.rnn_model(data[:, step, :], state)\n",
    "      if isinstance(next_state, tuple):\n",
    "        h, c = next_state\n",
    "        outputs = h\n",
    "      else:\n",
    "        outputs = next_state\n",
    "      state = next_state\n",
    "\n",
    "    logits = self.classifier(outputs)\n",
    "    return logits, state\n",
    "\n",
    "  def reset_parameters(self):\n",
    "    with torch.no_grad:\n",
    "      for param in self.parameters():\n",
    "        param.reset_parameters()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kHF55Pa-D4xc"
   },
   "outputs": [],
   "source": [
    "def shakespeare_trainer(batch_size, epochs):\n",
    "  train_dataset = ShakespeareDataset(encoded_text=shakespeare_text,\n",
    "                                     history_length=HISTORY_LENGTH)\n",
    "\n",
    "  print('Train dataset: %d' % len(train_dataset))\n",
    "\n",
    "  train_loader = DataLoader(train_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            num_workers=8)\n",
    "  vocabulary = train_dataset.get_vocabulary()\n",
    "\n",
    "  best_model = None\n",
    "  best_loss = 0.0\n",
    "  full_loss = []\n",
    "\n",
    "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "  model = SentenceGeneration(vocabulary_size=len(vocabulary),\n",
    "                             embedding_dim=EMBEDDING_DIM,\n",
    "                             rnn_module=RNN_MODULES[RNN_MODULE],\n",
    "                             hidden_size=HIDDEN_SIZE)\n",
    "  model.to(device)\n",
    "\n",
    "  print('Model Architecture:\\n%s' % model)\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    model.train()\n",
    "    dataset = train_dataset\n",
    "    data_loader = train_loader\n",
    "\n",
    "    progress_bar = tqdm(enumerate(data_loader), total=len(data_loader))\n",
    "    for step, (sequences, labels) in progress_bar:\n",
    "      total_step = epoch * len(data_loader) + step\n",
    "      sequences = sequences.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      outputs, _ = model(sequences)\n",
    "      _, preds = torch.max(outputs, 1)\n",
    "      loss = criterion(outputs, labels)\n",
    "      corrects = torch.sum(preds == labels.data)\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      progress_bar.set_description(\n",
    "          'Loss: %.4f, Accuracy: %.4f' %\n",
    "          (loss.item(), corrects.item() / len(labels)))\n",
    "      full_loss.append(loss.item())\n",
    "\n",
    "  state_dict = {\"model\": model.cpu().state_dict(),\n",
    "                \"vocabulary\": vocabulary}\n",
    "\n",
    "  return state_dict, full_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IhtpdJufFN6d"
   },
   "outputs": [],
   "source": [
    "final_model, loss = shakespeare_trainer(batch_size=BATCH_SIZE,\n",
    "                                        epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGHfkuR0QCRB"
   },
   "outputs": [],
   "source": [
    "### You can make a plot using matplotlib with loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DXHS1g2DsovI"
   },
   "outputs": [],
   "source": [
    "def sample_next_char_id(predicted_logits):\n",
    "  next_char_id = categorical.Categorical(logits=predicted_logits).sample()\n",
    "  return next_char_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2VqD2C1FNHAQ"
   },
   "outputs": [],
   "source": [
    "def shakespeare_writer(state_dict, start_string):\n",
    "  \"\"\"Generates new sentences using trained language model.\"\"\"\n",
    "  device = 'cpu'\n",
    "\n",
    "  vocabulary = state_dict['vocabulary']\n",
    "\n",
    "  char2index = {x: i for (i, x) in enumerate(vocabulary)}\n",
    "  index2char = {i: x for (i, x) in enumerate(vocabulary)}\n",
    "\n",
    "  inputs = torch.tensor([char2index[x] for x in start_string])\n",
    "  inputs = inputs.view(1, -1)\n",
    "\n",
    "  model = SentenceGeneration(vocabulary_size=len(vocabulary),\n",
    "                             embedding_dim=EMBEDDING_DIM,\n",
    "                             rnn_module=RNN_MODULES[RNN_MODULE],\n",
    "                             hidden_size=HIDDEN_SIZE)\n",
    "\n",
    "  model.load_state_dict(state_dict['model'])\n",
    "  model.eval()\n",
    "\n",
    "  generated_chars = []\n",
    "  #####################################################################\n",
    "  # Implement here for generating new sentence                        #\n",
    "  # Specifically, you need to iterate through the history and predict #\n",
    "  # next character; then you could take the predicted history as part #\n",
    "  # of history then repeat the process. The generation should be      #\n",
    "  # repeated for FLAGS.generation_length times.\n",
    "  raise NotImplementedError\n",
    "  #####################################################################\n",
    "\n",
    "  return start_string + ''.join(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ap_S3JkANJoL"
   },
   "outputs": [],
   "source": [
    "generated_text = shakespeare_writer(final_model, START_STRING)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "CS2078",
   "language": "python",
   "name": "cs2078"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
